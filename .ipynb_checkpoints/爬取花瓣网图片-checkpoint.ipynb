{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "class HuabanCrawler():\n",
    "    \"\"\" 抓去花瓣网上的图片 \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" 在当前文件夹下新建images文件夹存放抓取的图片 \"\"\"\n",
    "        self.homeUrl = \"http://huaban.com/favorite/beauty/\"\n",
    "        self.images = []\n",
    "        if not os.path.exists('./images'):\n",
    "            os.mkdir('./images')\n",
    "\n",
    "    def __load_homePage(self):\n",
    "        \"\"\" 加载主页面 \"\"\"\n",
    "        return requests.get(url = self.homeUrl).content\n",
    "\n",
    "    def __make_ajax_url(self, No):\n",
    "        \"\"\" 返回ajax请求的url \"\"\"\n",
    "        return self.homeUrl + \"?i5p998kw&max=\" + No + \"&limit=20&wfl=1\"\n",
    "\n",
    "    def __load_more(self, maxNo):\n",
    "        \"\"\" 刷新页面 \"\"\"\n",
    "        return requests.get(url = self.__make_ajax_url(maxNo)).content\n",
    "\n",
    "    def __process_data(self, htmlPage):\n",
    "        \"\"\" 从html页面中提取图片的信息 \"\"\"\n",
    "        prog = re.compile(r'app\\.page\\[\"pins\"\\].*')\n",
    "        appPins = prog.findall(htmlPage)\n",
    "        # 将js中的null定义为Python中的None\n",
    "        null = None\n",
    "        true = True\n",
    "        if appPins == []:\n",
    "            return None\n",
    "        result = eval(appPins[0][19:-1])\n",
    "        for i in result:\n",
    "            info = {}\n",
    "            info['id'] = str(i['pin_id'])\n",
    "            info['url'] = \"http://img.hb.aicdn.com/\" + i[\"file\"][\"key\"] + \"_fw658\"\n",
    "            if 'image' == i[\"file\"][\"type\"][:5]:\n",
    "                info['type'] = i[\"file\"][\"type\"][6:]\n",
    "            else:\n",
    "                info['type'] = 'NoName'\n",
    "            self.images.append(info)\n",
    "\n",
    "    def __save_image(self, imageName, content):\n",
    "        \"\"\" 保存图片 \"\"\"\n",
    "        with open(imageName, 'wb') as fp:\n",
    "            fp.write(content)\n",
    "\n",
    "    def get_image_info(self, num=20):\n",
    "        \"\"\" 得到图片信息 \"\"\"\n",
    "        self.__process_data(self.__load_homePage())\n",
    "        for i in range((num-1)/20):\n",
    "            self.__process_data(self.__load_more(self.images[-1]['id']))\n",
    "        return self.images\n",
    "\n",
    "    def down_images(self):\n",
    "        \"\"\" 下载图片 \"\"\"\n",
    "        print \"{} image will be download\".format(len(self.images))\n",
    "        for key, image in enumerate(self.images):\n",
    "            print 'download {0} ...'.format(key)\n",
    "            try:\n",
    "                req = requests.get(image[\"url\"])\n",
    "            except :\n",
    "                print 'error'\n",
    "            imageName = os.path.join(\"./images\", image[\"id\"] + \".\" + image[\"type\"])\n",
    "            self.__save_image(imageName, req.content)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hc = HuabanCrawler()\n",
    "    hc.get_image_info(200)\n",
    "hc.down_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "url = 'http://huaban.com/favorite/beauty/'\n",
    "\n",
    "\n",
    "def requestMain():\n",
    "    request = urllib2.Request(url)\n",
    "    request.headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "\n",
    "    }\n",
    "\n",
    "    html_doc = urllib2.urlopen(request)\n",
    "\n",
    "    print html_doc.getcode()\n",
    "    return html_doc\n",
    "\n",
    "\n",
    "def getPins():\n",
    "    html_doc = requestMain().read()\n",
    "\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser', from_encoding='utf-8')\n",
    "\n",
    "    pins = soup.find_all('a', href=re.compile(r\"/pins/\\d+/\"))\n",
    "    # print pins\n",
    "    huaban = 'http://huaban.com'\n",
    "    i = 0\n",
    "    for pin in pins:\n",
    "        pin_urls = huaban + pin['href']\n",
    "        req = urllib2.Request(pin_urls)\n",
    "        resp = urllib2.urlopen(req)\n",
    "        soup = BeautifulSoup(resp, 'html.parser', from_encoding='utf-8')\n",
    "        div_tag = soup.find_all('div', class_=\"image-holder\")\n",
    "        i = i+1\n",
    "        print i\n",
    "        for tag in div_tag:\n",
    "\n",
    "            img = tag.find('img')\n",
    "\n",
    "            link = 'http:'+img.get('src')\n",
    "\n",
    "            print link\n",
    "\n",
    "            a = requests.get(link)\n",
    "            imgname = i\n",
    "            #imgname = link.split('/')[-1]\n",
    "            with open(r'C:\\Users\\wuzhi_000\\Desktop\\Python\\py_scrapy\\image\\%s.jpg' % imgname, 'wb') as pic:\n",
    "                pic.write(a.content)\n",
    "             　　\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print getPins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sohu.com/a/148016029_714863"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
