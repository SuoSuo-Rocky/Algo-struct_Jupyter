{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取 一张图片， 并做持久化保存\n",
    "import requests\n",
    "url = 'https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=2381789298,2193118133&fm=26&gp=0.jpg'\n",
    "# 对于反爬机制--UA检测(对请求载体的检测)， 应对的反反爬策略为： 添加请求头信息 User-Agent.\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "}\n",
    "f = open('one.jpg','wb')\n",
    "img_data = requests.get(url=url,headers=header).content\n",
    "f.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则匹配 URL 中 指定的字符串，例： ID 或 一张图片的文件名\n",
    "import re\n",
    "import requests \n",
    "url = 'https://ss1.bdstatic.com/70cFvXSh_Q1YnxGkpoWK1HF6hhy/it/u=2381789298,2193118133&fm=26&gp=0.jpg'\n",
    "shi = re.search('.*y/it/u=(.*?),.*',url)\n",
    "print(shi.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取 搜狗网首页 源码并做持久化保存\n",
    "import requests \n",
    "url = 'https://www.sogou.com/'\n",
    "res = requests.get(url=url)\n",
    "pg_text = res.text\n",
    "with open('sogou.html','w') as f:\n",
    "    f.write(pg_text)\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#需求：爬取搜狗指定词条搜索后的页面数据,做持久化保存\n",
    "import requests\n",
    "url = 'https://www.sogou.com/web'\n",
    "# 用户输入指定条目\n",
    "wd = input('enter a word:')\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "}\n",
    "#封装参数\n",
    "param = {\n",
    "    'query':wd\n",
    "}\n",
    "response = requests.get(url=url,params=param,headers=header)\n",
    "\n",
    "page_text = response.content\n",
    "fileName = wd+'.html'\n",
    "with open(fileName,'wb') as fp:\n",
    "    fp.write(page_text)\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sou:dog\n",
      "{'errno': 0, 'data': [{'k': 'dog', 'v': 'n. 狗; 犬; 公狗; 公狐; 公狼; 赛狗; 灵𤟥赛; v. (长期)困扰，折磨，纠缠; 跟踪;'}, {'k': 'dogs', 'v': 'n. 狗; 犬; 公狗; 公狐; 公狼; 赛狗; 灵𤟥赛; v. (长期)困扰，折磨，纠缠; 跟踪;'}, {'k': 'doge', 'v': 'n. （美西部）孤犊，无母犊牛;'}, {'k': 'doggy', 'v': 'n. (儿语)小狗，汪汪，狗狗; adj. 狗的; 像狗一样的;'}, {'k': 'doggie', 'v': 'n. (儿语)狗狗;'}]}\n"
     ]
    }
   ],
   "source": [
    "# 爬取 百度翻译中  指定词条 后返回的数据\n",
    "# https://fanyi.baidu.com/translate\n",
    "# 分析得知， 翻译结果数据 为 动态加载，是 Ajax 请求，返回的为 JSON 字典数据\n",
    "import requests \n",
    "url = \"https://fanyi.baidu.com/sug\"\n",
    "main = input(\"sou:\")\n",
    "data = {\n",
    "    \"kw\": main\n",
    "}\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "}\n",
    "res = requests.post(url=url,data=data,headers=header)\n",
    "# 确保此请求 返回的数据为 JSON 数据，才可使用 .json() 方法 接受，否则报错\n",
    "pg_text = res.json()  \n",
    "print(pg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数量:2\n",
      "开始位置:2\n",
      "只有爱能让我生存\t7.5\thttps://movie.douban.com/subject/30139756/\n",
      "大人物\t6.3\thttps://movie.douban.com/subject/30158840/\n"
     ]
    }
   ],
   "source": [
    "#  爬取  豆瓣电影  (按照热度)\n",
    "# https://movie.douban.com/explore\n",
    "# 分析得知: 页面电影信息为动态加载，\n",
    "# 当滚轮触底或点击加载 更多时，发起 Ajax 请求，返回 JSON 字典电影数据\n",
    "import requests \n",
    "url = 'https://movie.douban.com/j/search_subjects'\n",
    "limit = int(input('数量:'))\n",
    "start = int(input('开始位置:'))\n",
    "# 构造 请求 URL 参数\n",
    "data = {\n",
    "    \"type\":\"movie\",\n",
    "    \"tag\": \"热门\",\n",
    "    \"sort\": \"recommend\",\n",
    "    \"page_limit\": limit,\n",
    "    \"page_start\": start,\n",
    "}\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "}\n",
    "res = requests.get(url=url,params=data,headers=header)\n",
    "# 接受 JSON 字典数据\n",
    "pg_text = res.json()\n",
    "# 做持久化保存\n",
    "f = open('豆瓣Movie','w')\n",
    "for s in pg_text['subjects']:\n",
    "    entry = s['title'] + \"\\t\" + s['rate'] + '\\t' + s['url']\n",
    "    print(entry)\n",
    "    f.write(entry + '\\r\\n')\n",
    "f.close()\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "起始页:2\n",
      "页数:3\n",
      "每页的数量:30\n"
     ]
    }
   ],
   "source": [
    "#需求：爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证  每家公司的  相关数据\n",
    "# http://125.35.6.84:81/xk/\n",
    "# 第一步：************************ 分析：*****判断页面数据是否为动态加载\n",
    "# 得知：\n",
    "# 首页数据 为 动态加载，直接爬取首页 获取不到想要信息\n",
    "# 得知 分页 url 为 Ajax 请求，只是 不同的页码 所带的参数不一样，返回的为 json 字典，\n",
    "# 且 每一家企业的 详情页 的数据 也为 动态加载 ,为 Ajax 请求，返回的 为 json 字典\n",
    "# 通过 分页 URL 返回到的 json数据，得到 每一家企业的 ID， \n",
    "# 得知 详情页的 URL 都一样，只是 不同的企业 所带的参数 ID 不一样。\n",
    "# 再通过 分页 URL获得 ID 构造 详情页的 URL \n",
    "# 首页 分页的 URL 地址,\n",
    "import requests \n",
    "pg_url = \"http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList\"\n",
    "busi_url = \"http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById\"\n",
    "start = int(input('起始页:'))\n",
    "num = int(input(\"页数:\"))\n",
    "step = int(input('每页的数量:'))/2\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "}\n",
    "\n",
    "busi_list = []\n",
    "f = open('company.html','w',encoding='utf-8')\n",
    "for pag in range(start,start+num):     # 循环 爬取 每一页的 URL地址，\n",
    "    pg_data = {\n",
    "        \"on\": \"true\",\n",
    "        \"page\": pag,\n",
    "        \"pageSize\": step,\n",
    "        \"productName\": \"\",\n",
    "        \"conditionType\": \"1\",\n",
    "        \"applyname\": \"\",\n",
    "        \"applysn\": \"\",\n",
    "    }\n",
    "# 爬取 分页 URL， 获得 JSON 数据\n",
    "    pg_list = requests.post(url=pg_url,data=pg_data,headers=header).json()\n",
    "    for p in pg_list['list']:\n",
    "        busi_data = {\n",
    "            \"id\": p[\"ID\"],\n",
    "        }\n",
    "# 爬取 详情页 URL, 获得 JSON 数据\n",
    "        busi = requests.post(url=busi_url,data=busi_data,headers=header).json()\n",
    "       \n",
    "        busi_list.append(str(busi) + \"\\r\\n\")\n",
    "# 将所有 企业的 信息 列表 数据 做持久化保存\n",
    "f.writelines(busi_list)\n",
    "f.close()\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 百度搜索 --嗅图--: 爬取 照片\n",
    "# 分析:\n",
    "# 得知 页面数据 为动态加载，\n",
    "# 当滚轮 触底 时 自动 触发 Ajax 请求，返回 JSON 数据。\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "pg_url = \"https://image.baidu.com/search/acjson?tn=resultjson_com&ipn=rj&ct=201326592&is=&fp=result&queryWord=嗅图&cl=2&lm=-1&ie=utf-8&oe=utf-8&adpicid=&st=&z=&ic=&hd=&latest=&copyright=&word=嗅图&s=&se=&tab=&width=&height=&face=&istype=&qc=&nc=&fr=&expermode=&force=&pn=150&rn=100&gsm=96&1558065706969=\"\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "}\n",
    "res = requests.get(url=pg_url,headers=header).json()\n",
    "url_list = []\n",
    "for s in res['data'][:60]:\n",
    "    img_url = s['hoverURL']\n",
    "    if img_url:\n",
    "        url_list.append(img_url)\n",
    "#         print(img_url)\n",
    "if not os.path.exists('./xiutu'):\n",
    "    os.mkdir('./xiutu')\n",
    "for m in url_list:\n",
    "    print('url=',m)\n",
    "    file_name ='xiutu/'+ re.search(\".*/it/u=(.*?),.*\",m).groups()[0] + '.jpg'\n",
    "    f = open(file_name,'wb')\n",
    "    img_data = requests.get(url=m,headers=header).content\n",
    "    f.write(img_data)\n",
    "    f.close()\n",
    "print('over')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
